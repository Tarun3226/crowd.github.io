{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context aware Neural network Model\n",
    "\n",
    "## Convert PyTorch Models to TensorFlow \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mayub/Github/onnx-tensorflow/onnx_tf/common/__init__.py:89: UserWarning: onnx_tf.common.get_outputs_names is deprecated. It will be removed in future release. Use TensorflowGraph.get_outputs_names instead.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from onnx_tf.backend import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/mayub/Github/Context-Aware_Crowd_Counting-pytorch/')\n",
    "import cannet as cann_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PARAM_PATH = '/home/ubuntu/mayub/Github/Context-Aware_Crowd_Counting-pytorch/cvpr2019_CAN_SHHA_353.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model= cann_model.CANNet()\n",
    "device=torch.device(\"cpu\")\n",
    "model.load_state_dict(torch.load(MODEL_PARAM_PATH, map_location=torch.device('cpu')))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Image for CANNet model (context aware model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_convert_img(image_path):\n",
    "    img=plt.imread(image_path)/255\n",
    "    print(len(img.shape))\n",
    "    if len(img.shape)==2:\n",
    "        # expand grayscale image to three channel.\n",
    "        img=img[:,:,np.newaxis]\n",
    "        img=np.concatenate((img,img,img),2)\n",
    "    print(img.shape[0])\n",
    "    print(img.shape[1])\n",
    "    ds_rows=int(img.shape[0]//8) # Downsampling to match model size\n",
    "    ds_cols=int(img.shape[1]//8)\n",
    "    print(ds_rows)\n",
    "    print(ds_cols)\n",
    "    img = cv2.resize(img,(ds_cols*8,ds_rows*8))\n",
    "    print(img.shape)\n",
    "    img=img.transpose((2,0,1)) # convert to order (channel,rows,cols)\n",
    "    img_tensor=torch.tensor(img,dtype=torch.float)\n",
    "    img_tensor=transforms.functional.normalize(img_tensor,mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    img_tensor=img_tensor.view(1,img_tensor.shape[0],img_tensor.shape[1],img_tensor.shape[2])\n",
    "    print(img.shape)\n",
    "    print(img_tensor.shape)\n",
    "    #img_tensor = np.expand_dims(img_tensor,axis  = 0)\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Input image dimentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input= read_and_convert_img('/home/ubuntu/CrowdSourcing_Projects/CSRNet-keras/data/shanghaitech/part_A_final/test_data/images/IMG_1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Prediction on sample image to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_input=image_input.to(device)\n",
    "et_dmap=model(image_input)\n",
    "print(et_dmap.data.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model in ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX format\n",
    "torch.onnx.export(model, image_input, './model_simple.onnx', input_names=['image_input'], output_names=['image_output'],operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the model for TensorFlow export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ONNX model and convert to TensorFlow format\n",
    "model_onnx = onnx.load('./model_simple.onnx')\n",
    "\n",
    "# tf_rep = prepare(model_onnx)\n",
    "\n",
    "# Export model as .pb file\n",
    "#tf_rep.export_graph('./model_simple.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_rep = prepare(model_onnx,device='CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAILS !! Aten op is not implemented in ONNX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-07384f013c94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprepare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_crowd_detection_mayub)",
   "language": "python",
   "name": "conda_crowd_detection_mayub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
